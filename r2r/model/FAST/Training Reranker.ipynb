{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading navigation graphs for 72 scans\n",
      "R2RBatch loaded with 17409 instructions, using splits: train,val_seen,val_unseen\n"
     ]
    }
   ],
   "source": [
    "from tasks.R2R.env import R2RBatch\n",
    "from utils import Tokenizer, read_vocab\n",
    "from vocab import TRAINVAL_VOCAB, TRAIN_VOCAB\n",
    "vocab = read_vocab(TRAIN_VOCAB)\n",
    "tok = Tokenizer(vocab)\n",
    "env = R2RBatch(['none'], batch_size=64, splits=['train','val_seen','val_unseen'],tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_train40False.json\n",
      "gold 0.8408718569698697\n",
      "oracle 0.9913099223591424\n",
      "cache_val_seen40False.json\n",
      "gold 0.6581782566111655\n",
      "oracle 0.9285014691478942\n",
      "cache_val_unseen40False.json\n",
      "gold 0.49638143891017456\n",
      "oracle 0.902085994040017\n"
     ]
    }
   ],
   "source": [
    "def average(_l):\n",
    "    return float(sum(_l)) / len(_l)\n",
    "\n",
    "def count_prefix_len(l1,l2):\n",
    "    res = 0\n",
    "    while(res < len(l1) and res < len(l2) and l1[res] == l2[res]):\n",
    "        res += 1\n",
    "    return res\n",
    "\n",
    "def get_path_len(scanId, path):\n",
    "    path_len = 0\n",
    "    prev = path[0]\n",
    "    for curr in path[1:]:\n",
    "        path_len += env.distances[scanId][prev][curr]\n",
    "\n",
    "def load_data(filenames):\n",
    "    all_data = []\n",
    "    for fn in filenames:\n",
    "        with open(fn,'r') as f:\n",
    "            train_file = json.loads(f.read())\n",
    "        train_instrs = list(train_file.keys())\n",
    "        train_data = {}\n",
    "        \n",
    "        for instr_id in train_instrs:\n",
    "            path_id = int(instr_id.split('_')[0])\n",
    "            scanId = env.gt[path_id]['scan']\n",
    "            new_data = {\n",
    "                'instr_id': instr_id,\n",
    "                'candidates': [],\n",
    "                'candidates_path': [],\n",
    "                'reranker_inputs': [],\n",
    "                'distance': [],\n",
    "                'gt': env.gt[path_id],\n",
    "                'gold_idx': -1,\n",
    "                'goal_viewpointId': env.gt[path_id]['path'][-1],\n",
    "                'gold_len': get_path_len(scanId, env.gt[path_id]['path']),\n",
    "            }\n",
    "            self_len = 0\n",
    "            for i, candidate in enumerate(train_file[instr_id]):\n",
    "                _, world_states, actions, sum_logits, mean_logits, sum_logp, mean_logp, pm, speaker, scorer = candidate\n",
    "                new_data['candidates'].append(candidate)\n",
    "                new_data['candidates_path'].append([ws[1] for ws in world_states])\n",
    "                new_data['reranker_inputs'].append([len(world_states), sum_logits, mean_logits, sum_logp, mean_logp, pm, speaker] * 4)\n",
    "                new_data['distance'].append(env.distances[scanId][world_states[-1][1]][new_data['goal_viewpointId']])\n",
    "                my_path = [ws[1] for ws in world_states]\n",
    "                if my_path == env.gt[path_id]['path']:\n",
    "                    new_data['gold_idx'] = i\n",
    "                \n",
    "            new_data['self_len'] = self_len\n",
    "            train_data[instr_id] = new_data\n",
    "            \n",
    "        print(fn)\n",
    "        print('gold',average([d['gold_idx'] != -1 for d in train_data.values()]))\n",
    "        print('oracle',average([any([dis < 3.0 for dis in d['distance']]) for d in train_data.values()]))\n",
    "        all_data.append(train_data)\n",
    "        \n",
    "    return all_data\n",
    "\n",
    "#[train_data, val_seen, val_unseen] = load_data(['cache_train40True.json','cache_val_seen40True.json','cache_val_unseen40True.json'])\n",
    "[train_data, val_seen, val_unseen] = load_data(['cache_train40False.json','cache_val_seen40False.json','cache_val_unseen40False.json'])\n",
    "#[train_data, val_unseen] = load_data(['cache_train40False.json','cache_val_unseen20False.json'])\n",
    "#[val_unseen] = load_data(['cache_val_unseen40False.json'])\n",
    "#data_dim = len(train_data.values()['reranker_inputs'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(28).cuda()\n",
    "#net.load_state_dict(torch.load('candidates_ranker_{}'.format(.6321)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4374829\n"
     ]
    }
   ],
   "source": [
    "batch_labels = []\n",
    "valid_points = 0\n",
    "\n",
    "for training_point in train_data.values():\n",
    "    labels = training_point['distance']\n",
    "    gold_idx = np.argmin(labels)\n",
    "    ac_len = len(labels)\n",
    "    choice = 1\n",
    "    x_1 = []\n",
    "    x_2 = []\n",
    "    if choice == 1:\n",
    "        for i in range(ac_len):\n",
    "            for j in range(ac_len):\n",
    "                if labels[i] <= 3.0 and labels[j] > 3.0:\n",
    "                    x_1.append(i)\n",
    "                    x_2.append(j)\n",
    "                    valid_points += 1\n",
    "    else:\n",
    "        for i in range(ac_len):\n",
    "            if labels[i] > 3.0:\n",
    "                x_1.append(gold_idx)\n",
    "                x_2.append(i)\n",
    "                valid_points += 1\n",
    "    batch_labels.append((x_1, x_2))\n",
    "\n",
    "print(valid_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 611.0856170654297\n",
      "train 0.9017024004558729\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 1 loss 611.3334204405546\n",
      "train 0.9017024004558729\n",
      "val_unseen 0.6270753512132823\n",
      "epoch 2 loss 611.2385745197535\n",
      "train 0.9018448607450673\n",
      "val_unseen 0.6270753512132823\n",
      "epoch 3 loss 611.1470371484756\n",
      "train 0.9017736306004701\n",
      "val_unseen 0.6270753512132823\n",
      "epoch 4 loss 611.0579750239849\n",
      "train 0.9016311703112757\n",
      "val_unseen 0.626649638143891\n",
      "epoch 5 loss 610.9714311957359\n",
      "train 0.9017736306004701\n",
      "val_unseen 0.626649638143891\n",
      "epoch 6 loss 610.8873623609543\n",
      "train 0.9020585511788589\n",
      "val_unseen 0.626649638143891\n",
      "epoch 7 loss 610.8054997324944\n",
      "train 0.9021297813234561\n",
      "val_unseen 0.6270753512132823\n",
      "epoch 8 loss 610.7258523404598\n",
      "train 0.9022010114680533\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 9 loss 610.6485915482044\n",
      "train 0.9024147019018448\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 10 loss 610.5733136832714\n",
      "train 0.9025571621910392\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 11 loss 610.4998899549246\n",
      "train 0.9027708526248308\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 12 loss 610.4285130500793\n",
      "train 0.9031270033478168\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 13 loss 610.3587838709354\n",
      "train 0.9033406937816084\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 14 loss 610.2908236533403\n",
      "train 0.9033406937816084\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 15 loss 610.224662154913\n",
      "train 0.9035543842153999\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 16 loss 610.1600338369608\n",
      "train 0.9039105349383859\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 17 loss 610.096767872572\n",
      "train 0.9041242253721775\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 18 loss 610.0350984930992\n",
      "train 0.9042666856613719\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 19 loss 609.9748823791742\n",
      "train 0.9042666856613719\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 20 loss 609.9160686731339\n",
      "train 0.9042666856613719\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 21 loss 609.8585017323494\n",
      "train 0.9041954555167747\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 22 loss 609.8021827936172\n",
      "train 0.9044803760951635\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 23 loss 609.7472683936357\n",
      "train 0.9045516062397607\n",
      "val_unseen 0.6275010642826735\n",
      "epoch 24 loss 609.6935262531042\n",
      "train 0.9048365268181494\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 25 loss 609.6406992375851\n",
      "train 0.9049789871073438\n",
      "val_unseen 0.6279267773520647\n",
      "epoch 26 loss 609.5889724493027\n",
      "train 0.905050217251941\n",
      "val_unseen 0.6283524904214559\n",
      "epoch 27 loss 609.5384231060743\n",
      "train 0.9049789871073438\n",
      "val_unseen 0.6283524904214559\n",
      "epoch 28 loss 609.4889588654041\n",
      "train 0.9052639076857326\n",
      "val_unseen 0.6283524904214559\n",
      "epoch 29 loss 609.4404676407576\n",
      "train 0.9052639076857326\n",
      "val_unseen 0.6287782034908471\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "x_1 = []\n",
    "x_2 = []\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00005, momentum=0.6)\n",
    "best_performance = 0.0\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    epoch_loss = 0\n",
    "    for i, (instr_id, training_point) in enumerate(train_data.items()):\n",
    "        inputs = training_point['reranker_inputs']\n",
    "        labels = training_point['distance']\n",
    "        ac_len = len(labels)\n",
    "        \n",
    "        inputs = torch.stack([torch.Tensor(r) for r in inputs]).cuda()\n",
    "        labels = torch.Tensor(labels)\n",
    "        scores = net(inputs)\n",
    "        \n",
    "        if i%10 == 0 and len(x_1):\n",
    "            x1 = torch.cat(x_1, 0)\n",
    "            x2 = torch.cat(x_2, 0)\n",
    "            loss = F.relu(1.0 - (x1 - x2)).mean()\n",
    "            #s = x1-x2\n",
    "            #loss = (-s + torch.log(1 + torch.exp(s))).mean()\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            x_1 = []\n",
    "            x_2 = []\n",
    "        \n",
    "        if len(batch_labels[i][0]) > 0:\n",
    "            x_1.append(scores[batch_labels[i][0]])\n",
    "            x_2.append(scores[batch_labels[i][1]])\n",
    "\n",
    "        \n",
    "    print('epoch', epoch, 'loss', epoch_loss)\n",
    "    \n",
    "    for env_name, data_dict in zip(['train','val_unseen'],[train_data, val_unseen]):\n",
    "        successes = []\n",
    "        for instr_id, point in data_dict.items():\n",
    "            inputs = point['reranker_inputs']\n",
    "            labels = point['distance']\n",
    "            inputs = torch.stack([torch.Tensor(r) for r in inputs]).cuda()\n",
    "            labels = torch.Tensor(labels)\n",
    "            scores = net(inputs)\n",
    "            pred = scores.max(0)[1].item()\n",
    "            successes.append(int(labels[pred] <= 3.0))\n",
    "        print(env_name, average(successes))\n",
    "        if env_name is 'val_unseen' and average(successes) > best_performance:\n",
    "            best_performance = average(successes)\n",
    "            torch.save(net.state_dict(), 'candidates_ranker_{}'.format(best_performance))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9015599401666785\n",
      "0.7878766293895577\n",
      "0.8578246313840017\n",
      "0.9125293824346463\n",
      "0.9160196595199088\n",
      "0.6671415342973146\n",
      "0.6999074008120236\n",
      "val_seen 0.7159647404505387\n",
      "0.6248775710088149\n",
      "0.6699314397649363\n",
      "0.6856023506366308\n",
      "0.693437806072478\n",
      "0.5367286973555337\n",
      "0.5377081292850147\n",
      "val_unseen 0.6402724563644104\n",
      "0.5649212430821626\n",
      "0.5440613026819924\n",
      "0.5615155385270327\n",
      "0.5874840357598978\n",
      "0.5014899957428693\n",
      "0.4367816091954023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "for env_name, data_dict in zip(['train','val_seen','val_unseen'],[train_data,val_seen,val_unseen]):\n",
    "    successes = []\n",
    "    inspect = [1,2,3,4,5,6]\n",
    "    other_success = [[] for _ in range(len(inspect))]\n",
    "    spl = []\n",
    "    for instr_id, point in data_dict.items():\n",
    "        inputs = point['reranker_inputs']\n",
    "        labels = point['distance']\n",
    "        inputs = torch.stack([torch.Tensor(r) for r in inputs]).cuda()\n",
    "        labels = torch.Tensor(labels)\n",
    "        scores = net(inputs)\n",
    "        pred = scores.max(0)[1].item()\n",
    "        successes.append(int(labels[pred] < 3.0))\n",
    "        \n",
    "        if (int(labels[pred] < 3.0)):\n",
    "            for i in range(len(point['distance'])):\n",
    "                pass\n",
    "                #print( point['reranker_inputs'][i])\n",
    "                #print( scores[i].item(), point['distance'][i], point['reranker_inputs'][i][5])\n",
    "            #print(\"\\n\")\n",
    "        \n",
    "        for idx,i in enumerate(inspect):\n",
    "            pred = np.argmax([_input[i] for _input in point['reranker_inputs']])\n",
    "            other_success[idx].append(int(labels[pred] < 3.0))\n",
    "        \n",
    "        #if labels[pred] < 3.0:\n",
    "        #    sp_len = get_path_len(env.gt[instr_id.split('_')[0]]['path'])\n",
    "        #    my_len = get_path_len()\n",
    "        #    spl.append(sp_len / my_len)\n",
    "        #else:\n",
    "        #    spl.append(0)\n",
    "    print(env_name, average(successes))\n",
    "    for idx in range(len(inspect)):\n",
    "        print(average(other_success[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perf_name = '{:.4f}'.format(average(successes))\n",
    "torch.save(net.state_dict(), 'candidates_ranker_{}'.format(perf_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
