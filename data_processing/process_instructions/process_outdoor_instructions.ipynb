{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mask/Replace Objects/Directions\"\"\"\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import regex\n",
    "import string\n",
    "import copy\n",
    "import stanza  # Use StanfordNLP POS Tagger.\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = regex.compile(r'(\\W+)')\n",
    "pos_tagger = stanza.Pipeline('en', processors='tokenize,mwt,pos')\n",
    "\n",
    "\n",
    "def check_dir(d):\n",
    "    if os.path.isdir(d):\n",
    "        print(d, '\\tEXISTS!')\n",
    "    else:\n",
    "        os.mkdir(d)\n",
    "        print(d, '\\tCREATED!')\n",
    "\n",
    "\n",
    "dataset = 'touchdown'\n",
    "src_data_dir = f'./raw_{dataset}/'\n",
    "dst_data_dir = f'../../{dataset}/data/'\n",
    "check_dir(dst_data_dir)\n",
    "input_filename_pattern = f'{src_data_dir}/%s.json'\n",
    "\n",
    "phases = ['train', 'dev', 'test']\n",
    "\n",
    "mask_token = '[MASK]'\n",
    "direction_groups = [['front', 'forward'], ['left'], ['right'], ['stop'], ['back']]\n",
    "directions = list(itertools.chain.from_iterable(direction_groups))\n",
    "numeric_groups = [\n",
    "    # cardinal\n",
    "    ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty'], \n",
    "    # ordinal\n",
    "    ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th', '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', '20th', \n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', 'seventeenth', 'eighteenth', 'nineteenth', 'twentieth']\n",
    "]\n",
    "numerics = list(itertools.chain.from_iterable(numeric_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    data = []\n",
    "    with open(datafile) as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    ''' Break sentence into a list of words and punctuation '''\n",
    "    toks = []\n",
    "    for word in [s.strip().lower() for s in SENTENCE_SPLIT_REGEX.split(sentence.strip()) if len(s.strip()) > 0]:\n",
    "        # Break up any words containing punctuation only, e.g. '!?', unless it is multiple full stops e.g. '..'\n",
    "        if all(c in string.punctuation for c in word) and not all(c in '.' for c in word):\n",
    "            toks += list(word)\n",
    "        else:\n",
    "            toks.append(word)\n",
    "    return ' '.join(toks)\n",
    "\n",
    "\n",
    "def load_objects():\n",
    "    objs = json.load(open(os.path.join(src_data_dir, f'objects.json')))\n",
    "    return objs\n",
    "\n",
    "\n",
    "def find_noun_positions(instr):\n",
    "    stopwords = directions\n",
    "    noun_positions = []\n",
    "    instr = tokenize_sentence(instr)\n",
    "    doc = pos_tagger(instr)\n",
    "    tokens = []\n",
    "    idx = 0\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            tokens.append(word.text)\n",
    "            # print(idx, '\\t', word.text, '\\t', word.pos)\n",
    "            if word.pos in ['NOUN', 'PROPN'] and word.text not in stopwords:\n",
    "                noun_positions.append(idx)\n",
    "            idx += 1\n",
    "            \n",
    "    return tokens, noun_positions\n",
    "\n",
    "\n",
    "def _process_objects(phase, mask_rate, intermediate, objects=None, replace=False, random_mask=False):\n",
    "    print('Processing %s-%.2f' % (phase, mask_rate))\n",
    "    rst = []\n",
    "\n",
    "    for item_ in tqdm(intermediate):\n",
    "        item = copy.deepcopy(item_)\n",
    "        token_group = copy.deepcopy(item['tokens'])\n",
    "        noun_position_group = item['noun_positions']\n",
    "        rst_instructions = []\n",
    "        for tokens, noun_positions in zip(token_group, noun_position_group):\n",
    "            if random_mask:  # controlled trial\n",
    "                mask_positions = random.sample(list(range(len(tokens))), int(mask_rate * len(noun_positions)))\n",
    "            else:\n",
    "                mask_positions = random.sample(noun_positions, int(mask_rate * len(noun_positions)))\n",
    "            for pos in mask_positions:\n",
    "                if replace:\n",
    "                    tokens[pos] = random.sample(objects, 1)[0]  # randomly select an object\n",
    "                else:  # mask\n",
    "                    tokens[pos] = mask_token\n",
    "            masked_instr = ' '.join(tokens)\n",
    "            rst_instructions.append(masked_instr)\n",
    "        item['navigation_text'] = rst_instructions[0]\n",
    "        del item['tokens']\n",
    "        del item['noun_positions']\n",
    "        rst.append(item)\n",
    "\n",
    "    print('#item:\\t', len(rst))\n",
    "    return rst\n",
    "\n",
    "\n",
    "def _find_replacing_direction(direction):\n",
    "    \"\"\"Find a replacement in the other groups\"\"\"\n",
    "    dg = direction_groups\n",
    "    for idx, group in enumerate(dg):\n",
    "        if direction in group:\n",
    "            break\n",
    "    cors = list(range(len(direction_groups)))\n",
    "    cors.remove(idx)\n",
    "    cands = []\n",
    "    for i in cors:\n",
    "        cands = cands + dg[i]\n",
    "    replacement = random.sample(cands, 1)[0]\n",
    "    return replacement\n",
    "\n",
    "\n",
    "def _process_directions(phase, mask_rate, intermediate, replace=False, random_mask=False):\n",
    "    print('Processing %s-%.2f' % (phase, mask_rate))\n",
    "    rst = []\n",
    "\n",
    "    for item_ in tqdm(intermediate):\n",
    "        item = copy.deepcopy(item_)\n",
    "        token_group = copy.deepcopy(item['tokens'])\n",
    "        rst_instructions = []\n",
    "        for tokens in token_group:\n",
    "            direction_positions = []\n",
    "            for pos, token in enumerate(tokens):\n",
    "                if token in directions:\n",
    "                    direction_positions.append(pos)\n",
    "            if random_mask:  # controlled trial\n",
    "                mask_positions = random.sample(list(range(len(tokens))), int(mask_rate * len(direction_positions)))\n",
    "            else:\n",
    "                mask_positions = random.sample(direction_positions, int(mask_rate * len(direction_positions)))\n",
    "            \n",
    "            for pos in mask_positions:\n",
    "                token = tokens[pos]\n",
    "                if replace:\n",
    "                    replacement = _find_replacing_direction(token)\n",
    "                    tokens[pos] = replacement\n",
    "                else:  # mask\n",
    "                    tokens[pos] = mask_token\n",
    "            masked_instr = ' '.join(tokens)\n",
    "            rst_instructions.append(masked_instr)\n",
    "\n",
    "        item['navigation_text'] = rst_instructions[0]\n",
    "        del item['tokens']\n",
    "        del item['noun_positions']\n",
    "        rst.append(item)\n",
    "\n",
    "    print('#item:\\t', len(rst))\n",
    "        \n",
    "    return rst\n",
    "\n",
    "\n",
    "def _find_replacing_numeric(numeric):\n",
    "    \"\"\"Find a replacement in the same group\"\"\"\n",
    "    for group in numeric_groups:\n",
    "        if numeric in group:\n",
    "            while True:\n",
    "                replacement = random.sample(group, 1)[0]\n",
    "                if replacement != numeric:\n",
    "                    break\n",
    "            break\n",
    "    return replacement\n",
    "\n",
    "\n",
    "def _process_numerics(phase, mask_rate, intermediate, replace=False, random_mask=False):\n",
    "    print('Processing %s-%.2f' % (phase, mask_rate))\n",
    "    rst = []\n",
    "\n",
    "    for item_ in tqdm(intermediate):\n",
    "        item = copy.deepcopy(item_)\n",
    "        token_group = copy.deepcopy(item['tokens'])\n",
    "        rst_instructions = []\n",
    "        for tokens in token_group:\n",
    "            numeric_positions = []\n",
    "            for pos, token in enumerate(tokens):\n",
    "                if token.lower() in numerics:\n",
    "                    numeric_positions.append(pos)\n",
    "            if random_mask:  # controlled trial\n",
    "                mask_positions = random.sample(list(range(len(tokens))), int(mask_rate * len(numeric_positions)))\n",
    "            else:\n",
    "                mask_positions = random.sample(numeric_positions, int(mask_rate * len(numeric_positions)))\n",
    "            \n",
    "            for pos in mask_positions:\n",
    "                token = tokens[pos]\n",
    "                if replace:\n",
    "                    replacement = _find_replacing_numeric(token.lower())\n",
    "                    tokens[pos] = replacement\n",
    "                else:  # mask\n",
    "                    tokens[pos] = mask_token\n",
    "            masked_instr = ' '.join(tokens)\n",
    "            rst_instructions.append(masked_instr)\n",
    "\n",
    "        item['navigation_text'] = rst_instructions[0]\n",
    "        del item['tokens']\n",
    "        rst.append(item)\n",
    "\n",
    "    print('#item:\\t', len(rst))\n",
    "        \n",
    "    return rst\n",
    "\n",
    "\n",
    "def _create_label(setting):\n",
    "    \"\"\"\n",
    "    Create label which is the abbrev of the setting.\n",
    "    'replace_object' -> 'ro'\n",
    "    \"\"\"\n",
    "    return ''.join(w[0] for w in setting.split('_'))\n",
    "\n",
    "\n",
    "def _save_result(rst, phase, mask_rate, setting, repeat_idx):\n",
    "    label = _create_label(setting)\n",
    "    cur_dst_dir = os.path.join(dst_data_dir, setting, f'{label}{mask_rate:.2f}_{repeat_idx}')\n",
    "    check_dir(os.path.join(dst_data_dir, setting))\n",
    "    check_dir(cur_dst_dir)\n",
    "    with open(os.path.join(cur_dst_dir, f'{phase}.json'), 'w') as fout:\n",
    "        for item in rst:\n",
    "            json.dump(item, fout)\n",
    "            fout.write('\\n')\n",
    "\n",
    "\n",
    "def process_object_and_direction_tokens(phase, data, mask_rates, objects=None, repeat_idx=0):\n",
    "    print('Processing %s' % phase)\n",
    "    random.seed(repeat_idx)\n",
    "    \n",
    "    intermediate_dir = os.path.join(src_data_dir, 'intermediate')\n",
    "    check_dir(intermediate_dir)\n",
    "    intermediate_filename = os.path.join(intermediate_dir, f'{phase}.json')\n",
    "    if os.path.exists(intermediate_filename):\n",
    "        intermediate = json.load(open(intermediate_filename, 'r'))\n",
    "        print('intermediate loaded from %s' % intermediate_filename)\n",
    "    else:\n",
    "        print('generating intermediate from scratch.')\n",
    "        intermediate = []\n",
    "        for item in tqdm(data):\n",
    "            instructions = [item['navigation_text']]\n",
    "            item['tokens'] = []\n",
    "            item['noun_positions'] = []\n",
    "            for instr in instructions:\n",
    "                tokens, noun_positions = find_noun_positions(instr)\n",
    "                item['tokens'].append(tokens)\n",
    "                item['noun_positions'].append(noun_positions)\n",
    "            intermediate.append(item)\n",
    "\n",
    "        with open(intermediate_filename , 'w') as fout:\n",
    "            json.dump(intermediate, fout)\n",
    "    \n",
    "    # Process by mask_rate\n",
    "    for mask_rate in mask_rates:\n",
    "        # OBJECT\n",
    "        rst = _process_objects(phase, mask_rate, intermediate)\n",
    "        _save_result(rst, phase, mask_rate, setting='mask_object', repeat_idx=repeat_idx)\n",
    "        \n",
    "        rst = _process_objects(phase, mask_rate, intermediate, objects, replace=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='replace_object', repeat_idx=repeat_idx)\n",
    "        \n",
    "        rst = _process_objects(phase, mask_rate, intermediate, random_mask=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='random_mask_for_object', repeat_idx=repeat_idx)  # controlled trial\n",
    "        \n",
    "        # DIRECTION\n",
    "        rst = _process_directions(phase, mask_rate, intermediate)\n",
    "        _save_result(rst, phase, mask_rate, setting='mask_direction', repeat_idx=repeat_idx)\n",
    "                \n",
    "        rst = _process_directions(phase, mask_rate, intermediate, replace=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='replace_direction', repeat_idx=repeat_idx)\n",
    "        \n",
    "        rst = _process_directions(phase, mask_rate, intermediate, random_mask=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='random_mask_for_direction', repeat_idx=repeat_idx)  # controlled trial\n",
    "\n",
    "\n",
    "def process_numeric_tokens(phase, data, mask_rates, repeat_idx=0):\n",
    "    print('Processing %s' % phase)\n",
    "    random.seed(repeat_idx)\n",
    "    \n",
    "    intermediate_dir = os.path.join(src_data_dir, 'intermediate')\n",
    "    check_dir(intermediate_dir)\n",
    "    intermediate_filename = os.path.join(intermediate_dir, f'{phase}_numeric.json')\n",
    "    if os.path.exists(intermediate_filename):\n",
    "        intermediate = json.load(open(intermediate_filename, 'r'))\n",
    "        print('intermediate loaded from %s' % intermediate_filename)\n",
    "    else:\n",
    "        print('generating intermediate for the subset containing numeric tokens from scratch.')\n",
    "        intermediate = []\n",
    "        subset = []\n",
    "        numeric_vocab = set(numerics)\n",
    "        for item in tqdm(data):\n",
    "            instructions = [item['navigation_text']]\n",
    "            item['tokens'] = []\n",
    "            contain_numeric = False\n",
    "            for instr in instructions:\n",
    "                instr = tokenize_sentence(instr)\n",
    "                item['tokens'].append(instr.split())\n",
    "                if numeric_vocab & set(instr.lower().split()):\n",
    "                    contain_numeric = True\n",
    "            if contain_numeric:\n",
    "                subset.append(item)\n",
    "                intermediate.append(item)\n",
    "\n",
    "        with open(intermediate_filename , 'w') as fout:\n",
    "            json.dump(intermediate, fout)\n",
    "            print('#item:\\t', len(intermediate))\n",
    "\n",
    "        # store the subset that contains numeric tokens in the instruction\n",
    "        cur_dst_dir = os.path.join(dst_data_dir, 'numeric_default')\n",
    "        check_dir(cur_dst_dir)\n",
    "        with open(os.path.join(cur_dst_dir, f'{phase}.json'), 'w') as fout:\n",
    "            for item in subset:\n",
    "                json.dump(item, fout)\n",
    "                fout.write('\\n')\n",
    "\n",
    "    print('#input_item:\\t', len(intermediate))\n",
    "\n",
    "    # Process by mask_rate\n",
    "    for mask_rate in mask_rates:\n",
    "        rst = _process_numerics(phase, mask_rate, intermediate)\n",
    "        _save_result(rst, phase, mask_rate, setting='mask_numeric', repeat_idx=repeat_idx)\n",
    "        \n",
    "        rst = _process_numerics(phase, mask_rate, intermediate, replace=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='replace_numeric', repeat_idx=repeat_idx)\n",
    "        \n",
    "        rst = _process_numerics(phase, mask_rate, intermediate, random_mask=True)\n",
    "        _save_result(rst, phase, mask_rate, setting='random_mask_for_numeric', repeat_idx=repeat_idx)  # controlled trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process Numeric Tokens\"\"\"\n",
    "for phase in phases:\n",
    "    data = load_data(input_filename_pattern % phase)\n",
    "    mask_rates = [1.] # [0.2 * i for i in range(6)]\n",
    "    for i in range(5):  # repeat 5 times\n",
    "        process_numeric_tokens(phase, data, mask_rates, repeat_idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process Object Tokens and Direction Tokens\"\"\"\n",
    "for phase in phases:\n",
    "    objects = load_objects()\n",
    "    data = load_data(input_filename_pattern % phase)\n",
    "    mask_rates = [1.] # [0.2 * i for i in range(6)]\n",
    "    for i in range(5):  # repeat 5 times\n",
    "        process_object_and_direction_tokens(phase, data, mask_rates, objects, repeat_idx=i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "179466156507d687c989ffd2833b5f4164c7156c2303d560050b1cf16cd93fcd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('envdrop': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
